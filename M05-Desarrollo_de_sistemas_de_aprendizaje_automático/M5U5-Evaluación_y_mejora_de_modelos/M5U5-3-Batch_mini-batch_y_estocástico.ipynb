{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb77f4a-5b33-473c-83d3-dfff59c18787",
   "metadata": {},
   "source": [
    "# Gradient descent por batch vs mini-batch vs estocástico\n",
    "M5U5 - Ejercicio 2\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Modificar nuestra implementación de batch gradient descent a mini-batch y estocástico\n",
    "- Comprobar las diferencias entre el entrenamiento según los 3 métodos\n",
    "\n",
    "Recuerda seguir las instrucciones para las entregas de prácticas indicadas en [Instrucciones entregas](https://github.com/Tokio-School/Machine-Learning/blob/main/Instrucciones%20entregas.md).\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "Hasta ahora hemos hablado de entrenamiento de modelos u optimización de funciones por descenso de gradiente. Sin embargo, hemos omitido que nos referíamos al descenso de gradiente de tipo \"batch\", que podríamos distinguir del mini-batch y del estocástico.\n",
    "\n",
    "Para una comparación en detalle de los 3 tipos puedes acudir al contenido del curso. Sirva recordar simplemente:\n",
    "- \"Batch\" = conjunto de datos de entrenamiento para 1 iteración, época o \"epoch\".\n",
    "- Iteración o \"epoch\": iteración sobre entrenamiento, bucle tras el cual se actualizan los pesos de $\\Theta$.\n",
    "- Batch: Una iteración o \"epoch\" sobre todos los datos de entrenamiento antes de actualizar $\\Theta$.\n",
    "    - Lento pero estable, converge finalmente.\n",
    "- Estocástico: Una iteración por cada ejemplo de entrenamiento.\n",
    "    - Rápido al inicio pero muy inestable, le cuesta mucho más converger. No se puede paralelizar.\n",
    "    - \"Estocástico\" ya que es mucho más aleatorio en su recorrido.\n",
    "- Mini-batch: Una iteración por partición de datos de entrenamiento, p. ej. 10% de datos o 10 particiones.\n",
    "    - Lo mejor de ambos mundos, más rápido que el batch, más estable que el estocástico, converge y se puede paralelizar.\n",
    "\n",
    "Vamos a implementar los 3 tipos, tanto de forma manual o personalizada con Numpy como con Scikit-learn, y comparar sus características, en este caso para **regresión lineal**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95baf0b-004f-4d83-b3c0-adeeb8ef64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Importa todas las librerías necesarias en esta celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c018a1-e07f-4ad9-8999-8a5bfa265d4c",
   "metadata": {},
   "source": [
    "## Generación de dataset sintético y procesado de datos\n",
    "\n",
    "Rescata tus celdas para crear un dataset sintético para regresión lineal, con métodos de Numpy o Scikit-learn:\n",
    "- Crea un dataset sin término de error\n",
    "- Reordena los datos aleatoriamente\n",
    "- Normaliza los datos si es necesario\n",
    "- Divide el dataset en subsets de entrenamiento y test, no haremos validación o regularización en este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8039c-69a3-427d-8739-47b92294479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crea un dataset sintético para regresión lineal sin término de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134b084-dcc4-4d8c-837c-a78580973233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reordena los datos de forma aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c1240-5fa7-4b50-b239-b24d8172b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normaliza los datos si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749814f-62d3-433b-a551-cf64f82a4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide el dataset en subsets de entrenamiento y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7eacb-b653-4d21-a916-08e54a92ef35",
   "metadata": {},
   "source": [
    "## Gradient descent personalizado\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "Recuerda las ecuaciones de la función de coste y del descenso de gradiente para el batch gradient descent regularizadas:\n",
    "\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$ J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$\n",
    "\n",
    "Vamos a recuperar la implementación de batch gradient descent que has utilizado en ejercicios anteriores para tomarla como base del gradient descent por mini-batches o estocástico.\n",
    "\n",
    "Comienza por recuperar las celdas de implementación de la función de coste, su comprobación de implementación, del gradient descent regularizado, del entrenamiento de un modelo y de la comprobación de su implementación.\n",
    "\n",
    "Una vez recuperadas, ejecuta las celdas, añadiéndole el sufijo `_batch` a las variables de la evolución de la función de coste y $\\Theta$ final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a05ff9-8102-41ce-8663-410a3d91f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que implementa la función de coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f1660-1207-47b9-8a78-e57de24adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que comprueba la implementación de la función de coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca227b-443f-44cd-9de7-b38eef903624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que implementa la función de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13618f49-0c9c-46a4-af5a-9ec8ce28d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que entrena un modelo con un dataset de entrenamiento y unos hiper-parámetros dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261b197-0204-4921-b5b5-42b7aca14070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que comprueba la implementación de la función de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a76627-b7d9-4db2-93a2-4141bdf45a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recupera la celda que representa gráficamente la evolución del historial de la función de coste vs las iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae4c9f-d67f-475d-9d5a-807338cd27ae",
   "metadata": {},
   "source": [
    "### Gradient descent estocástico\n",
    "\n",
    "En el gradient descent estocástico, actualizamos los valores de $\\Theta$ tras cada ejemplo, acabando una época cuando completamos una pasada por todos los ejemplos.\n",
    "\n",
    "Por tanto, el algoritmo de entrenamiento será:\n",
    "1. Reordenar los ejemplos de forma aleatoria (ya los hemos reordenado).\n",
    "1. Inicializar $\\Theta$ a valores aleatorios.\n",
    "1. Para cada época, hasta un nº máx. de iteraciones:\n",
    "    1. Por cada ejemplo de entrenamiento:\n",
    "        1. Computa la predicción o hipótesis $h_\\Theta(x^i)$\n",
    "        1. Computa el coste, pérdida o error de dicha predicción\n",
    "        1. Computa los gradientes de los coeficientes $\\Theta$\n",
    "        1. Actualiza los coeficientes $\\Theta$\n",
    "\n",
    "Por tanto, las ecuaciones de la función de coste y descenso de gradiente estocástico regularizadas son:\n",
    "\n",
    "$$ h_\\theta(x^i) = y^i = x^i \\times \\Theta^T $$\n",
    "$$ J_\\theta(x^i) = \\frac{1}{2m} [(h_\\theta(x^i) - y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} (h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} (h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c27ff7-04e6-4985-b06a-6ca60b396bef",
   "metadata": {},
   "source": [
    "Ahora adapta tu celda de entrenamiento de modelo para gradient descent estocástico y entrena un modelo sobre los datos de entrenamiento:\n",
    "\n",
    "*NOTA:* Intenta utilizar los mismos hiper-parámetros y Theta inicial para todos los modelos, para poder compararlos a posteriori en las mismas circunstancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cca92-56ef-425e-ab0c-18a6b5721d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapta la función de descenso de gradiente regularizado a estocástico\n",
    "# NOTA: Comprueba primero la implementación antes de modificarla. Puede que no sean necesarios muchos cambios..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764c690-d0e5-491a-87f1-8b20e30ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente estocástico\n",
    "# Añade el sufijo \"_estocastico\" a las variables resultado para distinguirlo de otros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c546362-d871-4413-8aa7-62671c3ece97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba la implementación del descenso de gradiente estocástico en varias circunstancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbff44-48d1-4b22-9334-7c44c4c2ed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente la evolución de la función de coste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e20112-4ae5-4939-baca-92ca07ea4576",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "En el gradient descent con mini-batches, actualizamos los valores de $\\Theta$ tras cada subconjunto de ejemplos o \"batch\", una partición del subset de entrenamiento, acabando una época cuando completamos una pasada por todos los \"batches\" o ejemplos.\n",
    "\n",
    "Por tanto, el algoritmo de entrenamiento será:\n",
    "1. Reordenar los ejemplos de forma aleatoria (ya los hemos reordenado).\n",
    "1. Para cada época, hasta un nº máx. de iteraciones:\n",
    "    1. Inicializar $\\Theta$ a valores aleatorios.\n",
    "    1. Dividir los ejemplos de entrenamiento en *k* \"batches\".\n",
    "    1. Por cada \"batch\":\n",
    "        1. Computa la predicción o hipótesis $h_\\Theta(x^i)$ sobre todo el \"batch\"\n",
    "        1. Computa el coste, pérdida o error de dicha predicción sobre el mismo\n",
    "        1. Computa los gradientes de los coeficientes $\\Theta$\n",
    "        1. Actualiza los coeficientes $\\Theta$\n",
    "\n",
    "Por tanto, las ecuaciones de la función de coste y descenso de gradiente con mini-batches regularizadas son:\n",
    "\n",
    "$$ m_k = \\text{nº de ejemplos en el \"batch\" actual} $$\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$ J_\\theta = \\frac{1}{2 m_k} [\\sum\\limits_{i=0}^{m_k} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j] $$\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m_k} \\sum_{i=0}^{m_k}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m_k}) - \\alpha \\frac{1}{m_k} \\sum_{i=0}^{m_k}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4373f-8917-4e94-850c-42e70b3bc22e",
   "metadata": {},
   "source": [
    "Ahora adapta tu celda de entrenamiento de modelo para gradient descent estocástico y entrena un modelo sobre los datos de entrenamiento:\n",
    "\n",
    "*NOTA:* Intenta utilizar los mismos hiper-parámetros y Theta inicial para todos los modelos, para poder compararlos a posteriori en las mismas circunstancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18be36-181e-4c72-ac57-2d8eda05cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adapta la función de descenso de gradiente regularizado a mini-batch\n",
    "# NOTA: Comprueba primero la implementación antes de modificarla. Puede que no sean necesarios muchos cambios..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60973b-058e-420a-8c2b-3aec81195c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente con mini-batches\n",
    "# Añade el sufijo \"_mini_batch\" a las variables resultado para distinguirlo de otros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e6fdc-76b5-4d1a-9517-18bca4e135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba la implementación del descenso de gradiente con mini-batches en varias circunstancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b4a8-98f8-4e0f-a74b-7747f6dd87a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente la evolución de la función de coste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da8cb5-f932-4be1-ad00-9b654a5878af",
   "metadata": {},
   "source": [
    "## Comparación de métodos\n",
    "\n",
    "Responde a las siguientes preguntas en la siguiente celda:\n",
    "*PREGUNTAS:*\n",
    "1. *¿Cuánto era necesario modificar las funciones de descenso de gradiente?*\n",
    "1. *¿Qué modelo ha tenido menor coste final?*\n",
    "1. *¿Qué modelo ha tardado menos tiempo en entrenarse/converger?*\n",
    "1. *¿Cómo han sido las evoluciones de la función de coste? ¿Han sido comparables en cuanto a estabilidad, p. ej.?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91fd760-2566-4e9b-97ea-07a88e783069",
   "metadata": {},
   "source": [
    "*RESPUESTAS:*\n",
    "1. ...\n",
    "1. ...\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e701d-2581-479e-85f3-c5cc3ec06a5c",
   "metadata": {},
   "source": [
    "### Comparación de residuos y precisión\n",
    "\n",
    "Calcula la precisión como RMSE y representa gráficamente los residuos de los 3 modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce243d-eff2-48be-bbc7-71b225102b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el RMSE de los 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc10e0-17ed-4762-9b39-a7b4ff0274ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente los residuos de los 3 modelos\n",
    "# Usa una gráfica de puntos con 3 series de colores diferentes y su leyenda\n",
    "# Incluye una rejilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb610a1-2425-4c0c-aeb4-888b304cc27a",
   "metadata": {},
   "source": [
    "*PREGUNTA:* ¿Aprecias diferencias entre ellos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7833fe-ace6-4523-8e05-4be630ab757e",
   "metadata": {},
   "source": [
    "## Gradient descent con Scikit-learn\n",
    "\n",
    "Ahora entrena 3 modelos y compara su rendimiento utilizando los métodos de Scikit-learn, en concreto regresión lineal por [linear_model.SGDRegressor](https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDRegressor.html) con sus métodos `fit()` y [partial_fit()](https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.partial_fit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18d642-93de-4664-bbf5-2eded37c99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente en batch con Scikit-learn\n",
    "# Añade el sufijo \"_batch\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c47c6-0147-45f0-98f2-5249d470c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente estocástico con Scikit-learn\n",
    "# Añade el sufijo \"_estocastico\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c4f4a-3fe0-47ad-803c-467eb2562274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entrena un modelo por descenso de gradiente coni mini-batches con Scikit-learn\n",
    "# Añade el sufijo \"_mini_batch\" a las variables resultado para distinguirlo de otros modelos\n",
    "# Muestra su tiempo de entrenamiento\n",
    "# Calcula su coste y RMSE final\n",
    "# Representa gráficamente sus residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40808a1-4c94-4cd3-8df2-2558577e73bf",
   "metadata": {},
   "source": [
    "*PREGUNTA:* ¿Aprecias diferencias entre ellos?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
