{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa1e653-43cc-498e-89d7-fcd9d96454bd",
   "metadata": {},
   "source": [
    "# Detección de anomalías: Dataset sintético\n",
    "M041- Ejercicio 1\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Crear un dataset sintético para detección de anomalías con casos normales y anómalos\n",
    "- Modelizar una distribución gaussiana sobre los datos normales\n",
    "- Determinar el umbral de probabilidad para detectar los datos anómalos por validación\n",
    "- Evaluar la precisión final del modelo sobre el subconjunto de test\n",
    "- Representar gráficamente el comportamiento del modelo en cada paso\n",
    "\n",
    "Recuerda seguir las instrucciones para las entregas de prácticas indicadas en [Instrucciones entregas](https://github.com/Tokio-School/Machine-Learning/blob/main/Instrucciones%20entregas.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fa1b3-a351-4231-8c86-ae3da364eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usa esta celda para importar todas las librerías necesarias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plot_n = 1\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5fc30-05e3-47cf-b356-6c4d456f6289",
   "metadata": {},
   "source": [
    "## Crear un dataset sintético para detección de anomalías\n",
    "\n",
    "Para resolver este ejercicio, primero necesitamos crear un dataset con datos normales y otro con datos anómalos. En este caso, los datasets serán 2D con 2 características únicamente, en lugar de un nº de características *n* elevado, para facilitar su visualización en una representación 2D.\n",
    "\n",
    "Inicialmente vamos a crear 2 datasets independientes, uno que representará los datos normales y otro los datos anómalos. Luego los combinaremos en 3 subsets finales de entrenamiento, validación y test, como habitualmente, con la particularidad de que en este caso los datos anómalos sólo se encontrarán en los subsets de validación y test.\n",
    "\n",
    "Completa la siguiente celda para crear los datasets iniciales independientes con datos normales y anómalos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d9f3f-f4a9-48b5-bc91-e539f54164d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera dos datasets sintéticos independientes con datos normales y anómalos\n",
    "\n",
    "m = 300\n",
    "n = 2\n",
    "ratio_anomalos = 0.15    # Porcentaje de datos anómalos vs datos normales, modificable\n",
    "m_anomalos = int(m * ratio_anomalos)\n",
    "m_normales = m - m_anomalos\n",
    "x_lim = (-5, 5)\n",
    "y_lim = (-5, 5)\n",
    "\n",
    "print('Nº de ejemplos: {}, ratio de ejemplos anómalos: {}%, nº de datos normales y anómalos: {}/{}'.format(m, ratio_anomalos * 100, m_normales, m_anomalos))\n",
    "print('Nº de características: {}'.format(n))\n",
    "\n",
    "# Creamos ambos datasets\n",
    "dataset_normales = make_blobs(n_samples=m_normales, centers=np.array([[1.5, 1.5]]), cluster_std=1.0, random_state=42)\n",
    "dataset_normales = dataset_normales[0]    # Descartamos el resto de información y retenemos sólo las posiciones de los ejemplos\n",
    "dataset_anomalos = np.random.uniform(low=(x_lim[0], y_lim[0]), high=(x_lim[1], y_lim[1]), size=(m_anomalos, 2))\n",
    "\n",
    "# Representamos los datos iniciales\n",
    "plt.figure(plot_n)\n",
    "\n",
    "plt.title('Dataset original: datos normales y anómalos')\n",
    "\n",
    "plt.scatter(dataset_normales[:, 0], dataset_normales[:, 1], s=10, color='b')\n",
    "plt.scatter(dataset_anomalos[:, 0], dataset_anomalos[:, 1], s=10, color='r')\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('Normales', 'Anómalos'))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22baf4a-d6ab-4cd7-9072-9ffafca2b219",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "### Normalización\n",
    "\n",
    "Antes de continuar, vamos a preprocesar los datos normalizándolos, como hacemos habitualmente. Dada que nuestra *X* se compondrá de datos normales y anómalos los normalizaremos a la vez.\n",
    "\n",
    "En este caso, no insertamos una primera columna de 1s de bias al dataset, por lo que normalizamos todas las columnas.\n",
    "\n",
    "Completa la siguiente celda de código para normalizar los datos, rescatando tu función de normalización de ejercicios anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd4008-bf7d-42a0-90e4-9bc4f1317923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normaliza los datos de ambos datasets con los mismos parámetros de normalización\n",
    "\n",
    "def normalize(x, mu, std):\n",
    "    return [...]\n",
    "\n",
    "# Halla la media y la desviación típica de las características de los datasets originales\n",
    "# Concatena antes ambos datasets en una X común, asegurándote de utilizar el eje correcto\n",
    "X = [...]\n",
    "mu_normalizar = [...]\n",
    "std = [...]\n",
    "\n",
    "print('Datasets originales:')\n",
    "print(dataset_normales.shape, dataset_anomalos.shape)\n",
    "\n",
    "print('Media y desviación típica de las características:')\n",
    "print(mu_normalizar)\n",
    "print(mu_normalizar.shape)\n",
    "print(std)\n",
    "print(std.shape)\n",
    "\n",
    "print('Datasets normalizados:')\n",
    "dataset_normales_norm = normalize(dataset_normales, mu_normalizar, std)\n",
    "dataset_anomalos_norm = normalize(dataset_anomalos, mu_normalizar, std)\n",
    "\n",
    "print(dataset_normales_norm.shape)\n",
    "print(dataset_anomalos_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8812f-ee9a-4d7c-beef-828a96530121",
   "metadata": {},
   "source": [
    "### Divsión en subsets de entrenamiento, validación y test\n",
    "\n",
    "Ahora vamos a subdividir los datasets originales en los subsets de entrenamiento, validación y test.\n",
    "\n",
    "Para ello, dividimos el dataset de datos normales según los ratios habituales, y asignamos la mitad de datos anómalos a los subsets de validación y test.\n",
    "\n",
    "Completa la siguiente celda de código para crear dichos subsets:\n",
    "\n",
    "*NOTA:* Recuerda que puedes hacerlo de forma manual o con los métodos de Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef12f5-cbdd-4c0b-bf75-7f09e7edd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Divide los datasets en los subsets de entrenamiento, validación y test con los datos normales y anómalos divididos entre los 2 últimos\n",
    "\n",
    "ratios = [66, 33, 33]\n",
    "print('Ratios:\\n', ratios, ratios[0] + ratios[1] + ratios[2])\n",
    "\n",
    "r = [0, 0]\n",
    "# Consejo: la función round() y el atributo x.shape pueden serte útiles\n",
    "r[0] = [...]\n",
    "r[1] = [...]\n",
    "print('Índices de corte:\\n', r)\n",
    "\n",
    "# Divide el dataset de datos normales en los 3 subsets siguiendo los ratios indicados\n",
    "X_train, X_val, X_test = [...]\n",
    "\n",
    "# Asigna la etiqueta de Y = 0 a todos los datos provenientes del dataset de datos normales e Y = 1 para los anómalos\n",
    "# Crea arrays 1D de la longitud del nº de ejemplos de cada subset con el valor de 0.\n",
    "Y_train = [...]\n",
    "\n",
    "# Ahora concatena la mitad de los datos anómalos al subset de validación y la otra mitad al de test\n",
    "dataset_anomalos_val, dataset_anomalos_test = [...]\n",
    "\n",
    "X_val = [...]\n",
    "X_test = [...]\n",
    "# El resultado final para X_val y X_test serán vectores 2D de (m_normales * ratio[validación o test] + m_anomalos / 2, n)\n",
    "\n",
    "# Por último, al igual que hemos hecho antes, concatena a Y_val e Y_test sendos arrays 1D con la longitud del nº de ejemplos anómalos en cada subset (la mitad de m_anomalos)\n",
    "# Cada array, en esta ocasión, tiene valores de 1. (float) en cada elemento\n",
    "Y_val = [...]\n",
    "Y_test = [...]\n",
    "# El resultado final para Y_val y Y_test serán vectores 1D de (m_normales * ratio[validación o test], 1) de 0s y (m_anomalos / 2, 1) de 1s.\n",
    "\n",
    "# Comprobamos los subsets creados\n",
    "print('Tamaños de los subsets de entrenamiento, validación y test:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978ddc2-48cd-4d30-b1e1-63ca2992370f",
   "metadata": {},
   "source": [
    "## Reordenación aleatoria de datos\n",
    "\n",
    "Por último, vamos a acabar de preprocesar los datasets reordenándolos aleatoriamente.\n",
    "\n",
    "Completa la siguiente celda de código para reordenarlos aleatoriamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb217c-872e-4024-b397-29ab32e2bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reordena aleatoriamente los subsets de entrenamiento, validación y test individualmente\n",
    "\n",
    "print('Primeras 10 filas y 2 columnas de X y vector Y:')\n",
    "print('Subset de entrenamiento:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2])\n",
    "print('Subset de validación:')\n",
    "print(X_val[:10,:2])\n",
    "print(Y_val[:10,:2])\n",
    "print('Subset de test:')\n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "\n",
    "print('Reordenamos X e Y:')\n",
    "# Usa un estado aleatorio inicial de 42, para mantener la reproducibilidad\n",
    "X_train, Y_train = [...]\n",
    "X_val, Y_val = [...]\n",
    "X_test, Y_test = [...]\n",
    "\n",
    "print('Primeras 10 filas y 2 columnas de X y vector Y:')\n",
    "print('Subset de entrenamiento:')\n",
    "print(X_train[:10,:2])\n",
    "print(Y_train[:10,:2])\n",
    "print('Subset de validación:')\n",
    "print(X_val[:10,:2])\n",
    "print(Y_val[:10,:2])\n",
    "print('Subset de test:')\n",
    "print(X_test[:10,:2])\n",
    "print(Y_test[:10,:2])\n",
    "\n",
    "print('Tamaños de los subsets de entrenamiento, validación y test:')\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565e020-b836-4348-b36a-099b6d724dd9",
   "metadata": {},
   "source": [
    "# Representación gráfica\n",
    "\n",
    "Por último, representamos nuestros 3 subsets en una gráfica 2D.\n",
    "\n",
    "Completa la siguiente celda de código para representar los subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c184a-f1d9-45e3-a669-b42f3e04899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa los 3 subsets en una gráfica 2D\n",
    "\n",
    "# Puedes ajustar los parámetros de matplotlib, como el rango de las dimensiones y tamaño de los puntos\n",
    "plt.figure(plot_n)\n",
    "\n",
    "plt.title('Subsets con datos normales y anómalos')\n",
    "\n",
    "cmap, norm = from_levels_and_colors([0., 0.5, 1.1], ['blue', 'red'])\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=25, c=Y_train, marker='o', cmap=cmap, norm=norm)\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], s=25, c=Y_val, marker='s', cmap=cmap, norm=norm)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=25, c=Y_test, marker='*', cmap=cmap, norm=norm)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim(y_lim)\n",
    "plt.legend(('Entrenamiento', 'Validación', 'Test'))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4573b-9463-4326-a745-075e04ca0c92",
   "metadata": {},
   "source": [
    "## Modelizar una distribución gaussiana\n",
    "\n",
    "Vamos a entrenar el modelo, que en este caso significará primero modelizar la presumible distribución gaussiana que siguen los datos normales.\n",
    "\n",
    "Inicialmente modelizamos sólo los datos normales para encontrar qué distribución siguen, qué datos son los normales o aceptables y cuáles no siguen dicha distribución y se deberían considerar anómalos.\n",
    "\n",
    "Una distibución gaussiana multivariable viene definida por 2 parámetros: la media $\\mu$ y la matriz de covarianza $\\Sigma$. $\\mu$ es un vector 1D de tamaño (*n*,) y $\\Sigma$ un vector 2D/matriz cuadarada (*n*, *n*).\n",
    "\n",
    "Recuerda del módulo y ejercicio sobre SVM con filtro gaussiano que la distribución gaussiana o normal multivariable tiene una forma redondeada u ovalada, donde la $\\mu$ representa la localización del punto central de la distribución en el espacio y la $\\Sigma$ la forma de la misma, más acentuada o picuda o menos.\n",
    "\n",
    "*NOTA:* Aunque la distribución normal o gaussiana es una de las más comunes en la naturaleza, en un proyecto real antes deberíamos comprobar si nuestros datos siguen una distribución normal o tenemos que modelarlos con otra distribución, siguiendo los mismos pasos.\n",
    "\n",
    "$\\mu$ y $\\Sigma$ se pueden calcular como:\n",
    "\n",
    "$$ \\mu = \\frac{1}{m} \\sum\\limits_{i=0}^{m} x^i; $$\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum\\limits_{i=0}^{m} (x^i - \\mu)(x^i - \\mu)^T; $$\n",
    "\n",
    "Sigue las siguientes instrucciones para modelizar la distribución gaussiana y obtener sus parámetros $\\mu$ y $\\Sigma$, y así luego poder calcular la probabilidad de que un punto sea normal o anómalo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74446758-c229-4cdf-a681-af8995fd7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modeliza la distribución gaussiana y obtén su mu y Sigma\n",
    "\n",
    "# Calcula la media y Sigma de X_train\n",
    "# Para ello, puedes utilizar las funciones de Numpy de media y matriz de covarianza con el eje adecuado\n",
    "mu = [...]\n",
    "sigma = [...]\n",
    "\n",
    "# Computa la distribución normal multivariable de los datos normales de entrenamiento X_train con dichos parámetros\n",
    "dist_normal = multivariate_normal(mean=mu, cov=sigma)\n",
    "\n",
    "print('Dimensiones de la media y matriz de covarianza del subset de entrenamiento:')\n",
    "print(mu.shape, sigma.shape)\n",
    "print('Media:')\n",
    "print(mu)\n",
    "print('Matriz de covarianza:')\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bacbc-c078-40bf-b16d-77323d4a115f",
   "metadata": {},
   "source": [
    "Vamos a representar gráficamente la función de densidad de la distribución de los datos normales con cortes de probabilidad junto al dataset de datos normales.\n",
    "\n",
    "Función de densidad de probabilidad:\n",
    "\n",
    "$$ pdf(x) = \\frac{1}{\\Sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2}(\\frac{x - \\mu}{\\Sigma})^2} $$\n",
    "\n",
    "Sigue las instrucciones de la siguiente celda para ello:\n",
    "\n",
    "*NOTA*: Puedes basarte en la función [contourf](https://matplotlib.org/stable/gallery/images_contours_and_fields/contourf_demo.html#sphx-glr-gallery-images-contours-and-fields-contourf-demo-py) de Matplotlib y en los ejemplos de [scipy.stats.multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy-stats-multivariate-normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636514-d2d8-4091-833d-487f38f048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa la función de densidad y los datos normales\n",
    "\n",
    "fig1, ax2 = plt.subplots([...])\n",
    "\n",
    "[...]\n",
    "\n",
    "# Añade una barra de color con la probabilidad de la distribución\n",
    "[...]\n",
    "\n",
    "# Añade un título y etiquetas a cada dimensión\n",
    "[...]\n",
    "\n",
    "# Representa también los datos del subset de entrenamiento X_train como puntos en la misma gráfica\n",
    "[...]\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b19fd-72f1-4edb-a334-dd732036dad5",
   "metadata": {},
   "source": [
    "## Determinar el umbral de probabilidad para detectar casos anómalos\n",
    "\n",
    "Ahora vamos a determinar el umbral de probabilidad $\\epsilon$ a partir del cual determinaremos si un nuevo caso es normal o anómalo. Si un ejemplo es demasiado diferente a los datos normales, si está alejado de ellos, si la probabilidad de que siga la misma distribución que los datos normales es inferior a dicho umbral, podemos declararlo como anómalo.\n",
    "\n",
    "Para encontrar dicho umbral, vamos a utilizar el subset de validación, con datos normales y anómalos, y al igual que la validación para la regularización en aprendizaje supervisado, vamos a estimar múltiples valores del umbral $\\epsilon$ quedándonos con aquel que mejor clasifique entre datos normales y anómalos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd1133-2549-4243-b6ce-c77c6ee7732b",
   "metadata": {},
   "source": [
    "Para comenzar, vamos a representar gráficamente la distribución, el subset de validación y varios posibles valores de $\\epsilon$ como fronteras de corte.\n",
    "\n",
    "Para ello, sigue las instrucciones para completar la siguiente celda:\n",
    "\n",
    "*NOTA*: Para las líneas de contorno, usa la función [contour](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.contour.html#matplotlib.axes.Axes.contour), también utilizada en el ejemplo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11053b88-8801-4821-84dc-1cbb6b7c874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa la distribución, el subset de validación y varios posibles valores de epsilon\n",
    "\n",
    "# Genera algunos valores para epsilon\n",
    "epsilon_evaluado = np.linspace(0., 0.5, num=10)\n",
    "\n",
    "# Recupera el código de la celda anterior y añádele una línea de contorno para cada valor de epsilon\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664998f6-5071-442b-a872-099ba11a040d",
   "metadata": {},
   "source": [
    "Para tener mayor visibilidad sobre la evaluación de nuestro dataset y comprobar el valor de $\\epsilon$ finalmente, vamos a computar las probabilidades de que cada dato anómalo del subset de validación siga la distribución de los datos normales.\n",
    "\n",
    "Para ello, sigue las instrucciones de la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d4788-e3b3-4572-92ae-231215ee4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las probabilidades de que los datos anómalos de validación sigan la distribución de entrenamiento\n",
    "\n",
    "# Filtra los datos del subset de validación que sean anómalos\n",
    "# Recuerda, los datos anómalos tienen una Y_val = 1.\n",
    "X_val_anomalos = [...]\n",
    "\n",
    "# Calcula sus probabilidades de que sigan la distribución normal\n",
    "p_val_anomalos = dist_normal.pdf(X_val_anomalos)\n",
    "\n",
    "print('Probabilidades de seguir la distribución normal de los 10 primeros datos anómalos de validación:')\n",
    "print(p_val_anomalos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793fdb3-06c2-4b33-bae4-5f141fee82a1",
   "metadata": {},
   "source": [
    "Estas probabilidades deben ser bastante bajas, para que queden prácticamente todas debajo del umbral de probabilidad o de corte de $\\epsilon$.\n",
    "\n",
    "Comprueba que ha sido así y también comprueba que la gran mayoría de datos no anómalos del subset de validación tienen probabilidades claramente superiores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96200dfa-352d-414a-96aa-4f3fc864a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las probabilidades de que los datos no anómalos de validacion sigan la distribución de entrenamiento\n",
    "\n",
    "# Filtra los datos del subset de validación que sean normales\n",
    "# Recuerda, los datos normales tienen una Y_val = 0.\n",
    "X_val_normales = [...]\n",
    "\n",
    "# Calcula sus probabilidades de que sigan la distribución normal\n",
    "p_val_normales = dist_normal.pdf(X_val_normales)\n",
    "\n",
    "print('Probabilidades de seguir la distribución normal de los 10 primeros datos normales de validación:')\n",
    "print(p_val_normales[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058d2bd-663c-4c5f-a460-e4ff1d1daffb",
   "metadata": {},
   "source": [
    "Para apreciarlo más fácilmente, representa ambas probabilidades de forma gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74926a21-ebea-4ba1-83a4-13ff65b7c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa gráficamente ambas probabilidades sobre todos los datos normales y anómalos como un gráfico de líneas y puntos\n",
    "\n",
    "plt.figure(plot_n)\n",
    "\n",
    "# Utiliza una leyenda y series con colores diferentes\n",
    "[...]\n",
    "\n",
    "plot_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9ed2d-ea69-45e8-ab30-6231ab3ec462",
   "metadata": {},
   "source": [
    "Por último, vamos a evaluar un espacio lineal de posibles valores de $\\epsilon$ y hallaremos el más óptimo para declarar un dato como anómalo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bedbd7-a692-45cc-ba7c-659dd2e941c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evalúa múltiples valores de epsilon y halla el más óptimo para clasificar datos como normales o anómalos\n",
    "\n",
    "# Genera un espacio lineal de valores de epsilon con más precisión\n",
    "epsilon_evaluado = np.linspace(0., 1., num=1e2)    # Puedes reducir la precisión para acelerar la computación\n",
    "\n",
    "# Valores a hallar su óptimo\n",
    "epsilon = 1e6    # Valor de epsilon inicial a optimizar\n",
    "f1_val = 0.    # F1_score de la clasificación\n",
    "for e in epsilon_evaluado:\n",
    "    # Asigna Y = 1. a valores cuya probabilidad sea inferior a epsilon y 0. al resto\n",
    "    Y_val_pred = np.where([...])\n",
    "    \n",
    "    # Halla el F1-score para dicha clasificación con Y_val como valor conocido\n",
    "    score = f1_score([...])\n",
    "    \n",
    "    if score > f1_val:\n",
    "        f1_val = score\n",
    "        epsilon = e\n",
    "\n",
    "print('Epsilon óptima en subset de validación:', epsilon)\n",
    "print('F1-score:', f1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f74abc-7bf8-4f60-b9dd-9a8e5bdd269d",
   "metadata": {},
   "source": [
    "Representa de nuevo líneas de contorno para las $\\epsilon$ evaluadas sobre una gráfica 2D con los datos de validación, normales y anómalos en distintos colores, y la $\\epsilon$ óptima con un color diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f93b9-322e-45c0-8db1-72401c7a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa de nuevo los datos y las líneas de corte para representar las epsilon evaluadas, la escogida finalmente y los datos normales y anómalos de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379a47d-5cb4-44b5-a4de-5285987b29ed",
   "metadata": {},
   "source": [
    "## Evaluar la precisión final del modelo\n",
    "\n",
    "Para finalizar nuestro entrenamiento, vamos a comprobar la precisión final del modelo sobre el subset de test, como hacemos habitualmente.\n",
    "\n",
    "Para ello, haremos una comprobación matemática y visual de dichos datos.\n",
    "\n",
    "Sigue las instrucciones para completar la siguiente celda y representar gráficamente los datos normales y anómalos del subset de test junto a la distribución de datos normales del subset de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aabb9-40d5-4d9e-8fb5-18d3729e0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa el subset de test junto a la distribución de datos del subset de entrenamiento\n",
    "# Incluye la línea de contorno para la epsilon escogida\n",
    "\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f2bf1-9fdc-4e2c-a2e5-483c0c3cd74c",
   "metadata": {},
   "source": [
    "Ahora calcula las métricas de evaluación de clasificación para evaluar la clasificación entre datos normales y anómalos que hace el modelo sobre el subset de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702ac8-7565-4356-9c3b-9d9c85b58ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula las métricas de evaluación de clasificación del modelo para el subset de test\n",
    "\n",
    "# Asigna Y = 1. a valores cuya probabilidad sea inferior a epsilon y 0. al resto\n",
    "Y_test_pred = np.where([...])\n",
    "\n",
    "# Haya el F1-score para la clasificación con Y_test como valor conocido\n",
    "f1_test = f1_score([...])\n",
    "\n",
    "print('F1-score para el subset de test:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a2158-146a-45d7-a106-0b5291f2a29d",
   "metadata": {},
   "source": [
    "Analiza gráficamente qué datos del subset de test clasifica correcta e incorrectamente el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea09ac-99ee-43f5-b860-91837a6f6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa errores y aciertos en el subset de test junto a la distribución y el corte de epsilon\n",
    "\n",
    "# Asigna z = 1. para acierto y z = 0. para fallo\n",
    "# Acierto: Y_test == Y_test_pred\n",
    "z = [...]\n",
    "\n",
    "# Representa la gráfica\n",
    "# Utiliza colores diferentes para los datos normales y anómalos y formas diferentes para los aciertos y fallos\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55549e42-0611-4676-869a-5efd331581ab",
   "metadata": {},
   "source": [
    "*¿Crees que el modelo hace un buen trabajo al detectar las anomalías?*\n",
    "\n",
    "*¿Hay algún dato que tú clasificarías de forma diferente?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0756999-3a84-4dcb-b5cd-501c8856cb1f",
   "metadata": {},
   "source": [
    "Por último, representa gráficamente todos los datos, de los 3 subsets, de forma conjunta, junto a la distribución y el corte $\\epsilon$, para analizar la distribución de datos normales y anómalos y el funcionamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5d4cd-c824-41af-86cc-e4ba97fad4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Representa los datos normales y anómalos junto a la distribución y el corte de epsilon\n",
    "# Representa los 3 subsets: entrenamiento, validación y test\n",
    "# Distingue los 3 subsets con marcadores de formas diferentes\n",
    "# Utiliza colores diferentes para datos normales y anómalos conocidos originalmente\n",
    "[...]\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
