{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b71ed5-8c08-462d-a3f3-bdfaa4b03bb5",
   "metadata": {},
   "source": [
    "# Regresión lineal: Regularización\n",
    "M2U3 - Ejercicio 2\n",
    "\n",
    "## ¿Qué vamos a hacer?\n",
    "- Implementar la función de coste regularizada para la regresión lineal multivariable\n",
    "- Implementar la regularización para el gradient descent\n",
    "\n",
    "Recuerda seguir las instrucciones para las entregas de prácticas indicadas en [Instrucciones entregas](https://github.com/Tokio-School/Machine-Learning/blob/main/Instrucciones%20entregas.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b9db2-3c8c-4412-8a3e-059b0a4bf801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8fa69-2cd8-4002-ab2c-13a5ae4ff396",
   "metadata": {},
   "source": [
    "## Creación de un dataset sintético\n",
    "\n",
    "Para comprobar tu implementación de una función de coste y gradient descent regularizado, rescata tus celdas de los notebooks anteriores acerca de datasets sintéticos y genera un dataset para este ejercicio.\n",
    "\n",
    "No olvides añadirle un término de bias a la *X* y un término de error a la *Y*, inicializado a 0 por ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b400b5-27b7-4ed9-b950-1475b0d722a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Genera un dataset sintéitico manualmente, con término de bias y término de error inicializado a 0\n",
    "\n",
    "m = 1000\n",
    "n = 3\n",
    "\n",
    "X = [...]\n",
    "\n",
    "Theta_verd = [...]\n",
    "\n",
    "error = 0.\n",
    "\n",
    "Y = [...]\n",
    "\n",
    "# Comprueba los valores y dimensiones de los vectores\n",
    "print('Theta a estimar y sus dimensiones:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Primeras 10 filas y 5 columnas de X e Y:')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print('Dimensiones de X e Y:')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1e349-ffff-4df9-9597-17e6b71a7806",
   "metadata": {},
   "source": [
    "## Función de coste regularizada\n",
    "\n",
    "Ahora vamos a modificar nuestra implementación de la función de coste de ejercicios anteriores para añadirle el término de regularización.\n",
    "\n",
    "Recuerda que la función de coste regularizada es:\n",
    "\n",
    "$$ h_\\theta(x^i) = Y = X \\times \\Theta^T $$\n",
    "$$J_\\theta = \\frac{1}{2m} [\\sum\\limits_{i=0}^{m} (h_\\theta(x^i)-y^i)^2 + \\lambda \\sum\\limits_{j=1}^{n} \\theta^2_j]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f2237-8286-40db-bf9c-14e5da6b298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementa la función de coste regularizada siguiendo la siguiente plantilla\n",
    "\n",
    "def regularized_cost_function(x, y, theta, lambda_=0.):\n",
    "    \"\"\" Computa la función de coste para el dataset y coeficientes considerados.\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    \n",
    "    Argumentos nombrados:\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    \n",
    "    Devuelve:\n",
    "    j -- float con el coste regularizada para dicho array theta\n",
    "    \"\"\"\n",
    "    m = [...]\n",
    "    \n",
    "    # Recuerda comprobar las dimensiones de la multiplicación matricial para hacerla correctamente\n",
    "    # Recuerda no regularizar el coeficiente del parámetro bias (primer valor de theta)\n",
    "    j = [...]\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd42e2-4677-4aab-b76c-f48a2d89c52b",
   "metadata": {},
   "source": [
    "*NOTA:* Comprueba que la función devuelve un valor float simplemente, y no un array o matriz. Utiliza el método `ndarray.resize((size0, size1))` si necesitas cambiar las dimensiones de cualquier array antes de multliplicarlo con `np.matmul()` y asegúrate que las dimensiones del resultado cuadren, o devuelve `j[0,0]` como valor `float`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71362cca-7836-4e76-b05e-1f8036b2566a",
   "metadata": {},
   "source": [
    "Como el dataset sintético tiene el término de error a 0, el resultado de la función de coste para la *Theta_verd* con parámetro *lambda* = 0 debe ser exactamente 0.\n",
    "\n",
    "Al igual que antes, según nos alejamos con valores de $\\theta$ diferentes, el coste debe aumentar. Del mismo modo, a mayor parámetro de regularización *lambda*, mayor penalización y coste, y a mayor valor de *Theta*, también mayor penalización y coste.\n",
    "\n",
    "Comprueba tu implementación en estas 5 circunstancias:\n",
    "1. Usando *Theta_verd* y con *lambda* a 0, el coste debe seguir siendo 0.\n",
    "1. Con *lambda* 0 aún, según los valores de *theta* se alejen de *Theta_verd*, el coste debe ser mayor.\n",
    "1. Usando *Theta_verd* y con *lambda* distinta de 0, el coste ahora debe ser mayor de 0.\n",
    "1. Con *lambda* distinta de 0, para una *theta* distinta a *Theta_verd* el coste debe ser mayor que con *lambda* igual a 0.\n",
    "1. Con *lambda* distinta de 0, cuanto mayores sean los valores de los coeficientes de *theta* (en sentido positivo o negativo), mayor será la penalización y el coste.\n",
    "\n",
    "Recordamos que el valor de *lambda* siempre debe ser positivo y generalmente menor de 0: `[0, 1e-1, 3e-1, 1e-2, 3e-2, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11d5ff-ea2c-4dbb-b8f0-b01801dec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba la implementación de tu función de coste regularizada en dichas circunstancias\n",
    "\n",
    "theta = Theta_verd    # Modifica y comprueba varios valores de theta\n",
    "\n",
    "j = regularized_cost_function(X, Y, theta)\n",
    "\n",
    "print('Coste del modelo:')\n",
    "print(j)\n",
    "print('Theta comprobada y Theta real:')\n",
    "print(theta)\n",
    "print(Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3de9e-cea7-4544-bc01-d36e56fb8a00",
   "metadata": {},
   "source": [
    "Anota tus experimentos y resultados en esta celda (en Markdown o código):\n",
    "1. Experimento 1\n",
    "1. Experimento 2\n",
    "1. Experimento 3\n",
    "1. Experimento 4\n",
    "1. Experimento 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9b45f-a805-471e-bcf2-ed4b978e77a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient descent regularizado\n",
    "\n",
    "Ahora vamos a regularizar también el entrenamiento por gradient descent. Vamos a modificar las actualizaciones de *Theta* para que ahora contengan también el parámetro de regularización *lambda*:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_0^i $$\n",
    "$$ \\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i + \\frac{\\lambda}{m} \\theta_j]; \\space j \\in [1, n] $$\n",
    "$$ \\theta_j := \\theta_j (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=0}^{m}(h_\\theta (x^i) - y^i) x_j^i; \\space j \\in [1, n] $$\n",
    "\n",
    "Recuerda basarte de nuevo en tu implementación anterior de la función de descenso de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bbcb9-5f11-4284-8d6c-2c673ef60051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar la función que entrena el modelo por gradient descent regularizado\n",
    "\n",
    "def regularized_gradient_descent(x, y, theta, alpha, lambda_=0., e, iter_):\n",
    "    \"\"\" Entrena el modelo optimizando su función de coste por gradient descent\n",
    "    \n",
    "    Argumentos posicionales:\n",
    "    x -- array 2D de Numpy con los valores de las variables independientes de los ejemplos, de tamaño m x n\n",
    "    y -- array 1D de Numpy con la variable dependiente/objetivo, de tamaño m x 1\n",
    "    theta -- array 1D de Numpy con los pesos de los coeficientes del modelo, de tamaño 1 x n (vector fila)\n",
    "    alpha -- float, ratio de entrenamiento\n",
    "    \n",
    "    Argumentos nombrados (keyword):\n",
    "    lambda -- float con el parámetro de regularización\n",
    "    e -- float, diferencia mínima entre iteraciones para declarar que el entrenamiento ha convergido finalmente\n",
    "    iter_ -- int/float, nº de iteraciones\n",
    "    \n",
    "    Devuelve:\n",
    "    j_hist -- list/array con la evolución de la función de coste durante el entrenamiento\n",
    "    theta -- array de Numpy con el valor de theta en la última iteración\n",
    "    \"\"\"\n",
    "    # TODO: declara unos valores por defecto para e e iter_ en los argumentos nombrados (keyword) de la función\n",
    "    \n",
    "    iter_ = int(iter_)    # Si has declarado iter_ en notación científica (1e3) o float (1000.), conviértelo\n",
    "    \n",
    "    # Inicializa j_hist como una list o un array de Numpy. Recuerda que no sabemos qué tamaño tendrá finalmente\n",
    "    j_hist = [...]\n",
    "    \n",
    "    m, n = [...]    # Obtén m y n a partir de las dimensiones de X\n",
    "    \n",
    "    for k in [...]:    # Itera sobre el nº de iteraciones máximo\n",
    "        # Declara una theta para cada iteración como \"deep copy\" de theta, ya que debemos actualizarla valor a valor\n",
    "        theta_iter = [...]\n",
    "        \n",
    "        for j in [...]:    # Itera sobre el nº de características\n",
    "            # Actualiza theta_iter para cada característica, según la derivada de la función de coste\n",
    "            # Incluye el ratio de entrenamiento alpha\n",
    "            # Cuidado con las multiplicaciones matriciales, su órden y dimensiones\n",
    "            \n",
    "            if j > 0:\n",
    "                # Regulariza todo coeficiente excepto el del parámetro bias (primer coef.)\n",
    "                pass\n",
    "            \n",
    "            theta_iter[j] = theta[j] - [...]\n",
    "            \n",
    "        theta = theta_iter\n",
    "        \n",
    "        cost = cost_function([...])    # Calcula el coste para la iteración de theta actual\n",
    "        \n",
    "        j_hist[...]    # Añade el coste de la iteración actual al histórico de costes\n",
    "        \n",
    "        # Comprueba si la diferencia entre el coste de la iteración actual y el de la última iteración en valor\n",
    "        # absoluto son menores que la diferencia mínima para declarar convergencia, e\n",
    "        if k > 0 and [...]:\n",
    "            print('Converge en la iteración nº: ', k)\n",
    "            \n",
    "            break\n",
    "    else:\n",
    "        print('Nº máx. de iteraciones alcanzado')\n",
    "        \n",
    "    return j_hist, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e39cc-536d-46fa-9ec1-e7fa4b654803",
   "metadata": {},
   "source": [
    "*Nota*: Recuerda que las plantillas de código son sólo una ayuda. En ocasiones, puede que quieras usar un código diferente con la misma funcionalidad, p. ej. que itere sobre los elementos de otra forma, etc. ¡Síentete libre de modificarlos a tu antojo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfea771-4dc6-45ed-824d-194542993aa2",
   "metadata": {},
   "source": [
    "## Comprobación del gradient descent regularizado\n",
    "\n",
    "Para comprobar tu implementación de nuevo, comprueba con *lambda* a 0 usando varios valores de *theta_ini*, tanto con la *Theta_verd* como valores cada vez más alejados de ella, y comprueba que finalmente el modelo converge a la *Theta_verd*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d91e9-3a92-4a13-91aa-261f2284c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comprueba tu implementación entrenando un modelo sobre el dataset sintético creado previamente\n",
    "\n",
    "# Crea una theta inicial con un valor dado, aleatorio o escogido a mano\n",
    "theta_ini = [...]\n",
    "\n",
    "print('Theta inicial:')\n",
    "print(theta_ini)\n",
    "\n",
    "alpha = 1e-1\n",
    "lambda_ = 0.\n",
    "e = 1e-3\n",
    "iter_ = 1e3    # Comprueba que tu función puede admitir valores float o modifícalo\n",
    "\n",
    "print('Hiper-arámetros usados:')\n",
    "print('Alpha:', alpha, 'Error máx.:', e, 'Nº iter', iter_)\n",
    "\n",
    "t = time.time()\n",
    "j_hist, theta_final = regularized_gradient_descent([...])\n",
    "\n",
    "print('Tiempo de entrenamiento (s):', time.time() - t)\n",
    "\n",
    "# TODO: completar\n",
    "print('\\nÚltimos 10 valores de la función de coste')\n",
    "print(j_hist[...])\n",
    "print('\\Coste final:')\n",
    "print(j_hist[...])\n",
    "print('\\nTheta final:')\n",
    "print(theta_final)\n",
    "\n",
    "print('Valores verdaderos de Theta y diferencia con valores entrenados:')\n",
    "print(Theta_verd)\n",
    "print(theta_final - Theta_verd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4affe7-77f8-475f-9f65-6bdf11d7a05d",
   "metadata": {},
   "source": [
    "Ahora comprueba de nuevo el entrenamiento de un modelo en algunas de las circunstancias anteriores:\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* a 0, el coste final debe seguir siendo cercano a 0 y la *theta* final cercana a *Theta_verd*.\n",
    "1. Usando una *theta_ini* aleatoria y con *lambda* pequeña y distinta de 0, el coste final debe ser cercano a 0, aunque el modelo puede empezar a tener peor precisión.\n",
    "1. Según aumenta el valor de *lambda*, el modelo perderá más precisión.\n",
    "\n",
    "Para ello recuerda que puedes modificar los valores de las celdas y reejecutarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677020e7-cc8d-4cb8-830a-58ac8aef8fa7",
   "metadata": {},
   "source": [
    "Anota tus experimentos y resultados en esta celda (en Markdown o código):\n",
    "1. Experimento 1\n",
    "1. Experimento 2\n",
    "1. Experimento 3\n",
    "1. Experimento 4\n",
    "1. Experimento 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccd072-2f3d-46d1-8663-5ec01b5ad3c7",
   "metadata": {},
   "source": [
    "## ¿Por qué necesitábamos utilizar regularización?\n",
    "\n",
    "El objetivo de la regularización era penalizar el modelo cuando sufre sobre-ajuste, cuando el modelo comienza a memorizar resultados más que aprender a generalizar.\n",
    "\n",
    "Ésto supone un problema cuando los datos de entrenamiento y sobre los que debemos hacer predicciones en producción siguen distribuciones significativamente diferentes.\n",
    "\n",
    "Para comprobar nuestro entrenamiento con descenso de gradiente regularizado, vuelve al apartado de generación del dataset y genera uno con un ratio de ejemplos a características bastante menor y con un ratio de error bastante superior.\n",
    "\n",
    "Comienza a jugar con dichos valores y luego ve modificando la *lambda* del modelo para ver si un valor de *lambda* diferente a 0 comienza a tener más precisión que *lambda* = 0."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
