{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EJEMPLO 1"
      ],
      "metadata": {
        "id": "uR4o7FxB7WhO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJjEF9W-rzKc"
      },
      "outputs": [],
      "source": [
        "#importamos librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#cargamos los datos de entrada\n",
        "dataframe = pd.read_csv(r\"comprar_alquilar.csv\")\n",
        "print(dataframe.tail(10))\n",
        "\n",
        "#normalizamos los datos\n",
        "scaler=StandardScaler()\n",
        "df = dataframe.drop(['comprar'], axis=1) # quito la variable dependiente \"Y\"\n",
        "scaler.fit(df) # calculo la media para poder hacer la transformacion\n",
        "X_scaled=scaler.transform(df)# Ahora si, escalo los datos y los normalizo\n",
        "\n",
        "#Instanciamos objeto PCA y aplicamos\n",
        "pca=PCA(n_components=9) # Otra opción es instanciar pca sólo con dimensiones nuevas hasta obtener un mínimo \"explicado\" ej.: pca=PCA(.85)\n",
        "pca.fit(X_scaled) # obtener los componentes principales\n",
        "X_pca=pca.transform(X_scaled) # convertimos nuestros datos con las nuevas dimensiones de PCA\n",
        "\n",
        "print(\"shape of X_pca\", X_pca.shape)\n",
        "expl = pca.explained_variance_ratio_\n",
        "print(expl)\n",
        "print('suma:',sum(expl[0:5]))\n",
        "#Vemos que con 5 componentes tenemos algo mas del 85% de varianza explicada\n",
        "\n",
        "#graficamos el acumulado de varianza explicada en las nuevas dimensiones\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.show()\n",
        "\n",
        "#graficamos en 2 Dimensiones, tomando los 2 primeros componentes principales\n",
        "Xax=X_pca[:,0]\n",
        "Yax=X_pca[:,1]\n",
        "labels=dataframe['comprar'].values\n",
        "cdict={0:'red',1:'green'}\n",
        "labl={0:'Alquilar',1:'Comprar'}\n",
        "marker={0:'*',1:'o'}\n",
        "alpha={0:.3, 1:.5}\n",
        "fig,ax=plt.subplots(figsize=(7,5))\n",
        "fig.patch.set_facecolor('white')\n",
        "for l in np.unique(labels):\n",
        "    ix=np.where(labels==l)\n",
        "    ax.scatter(Xax[ix],Yax[ix],c=cdict[l],label=labl[l],s=40,marker=marker[l],alpha=alpha[l])\n",
        "\n",
        "plt.xlabel(\"First Principal Component\",fontsize=14)\n",
        "plt.ylabel(\"Second Principal Component\",fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJEMPLO 2"
      ],
      "metadata": {
        "id": "pqjjwy8U7ZP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "from matplotlib import style\n",
        "style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "N-jptd8d7bEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USArrests = sm.datasets.get_rdataset(\"USArrests\", \"datasets\")\n",
        "datos = USArrests.data"
      ],
      "metadata": {
        "id": "fW_YhMbJ_JRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos.head(4)"
      ],
      "metadata": {
        "id": "IVNG0S_9_Nuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------')\n",
        "print('Media de cada variable')\n",
        "print('----------------------')\n",
        "datos.mean(axis=0)"
      ],
      "metadata": {
        "id": "O0Ei-K8Y_YQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-------------------------')\n",
        "print('Varianza de cada variable')\n",
        "print('-------------------------')\n",
        "datos.var(axis=0)"
      ],
      "metadata": {
        "id": "IaCOtWRe_4hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento modelo PCA con escalado de los datos\n",
        "# ==============================================================================\n",
        "pca_pipe = make_pipeline(StandardScaler(), PCA())\n",
        "pca_pipe.fit(datos)\n",
        "\n",
        "# Se extrae el modelo entrenado del pipeline\n",
        "modelo_pca = pca_pipe.named_steps['pca']"
      ],
      "metadata": {
        "id": "bb8JUKvt_63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se combierte el array a dataframe para añadir nombres a los ejes.\n",
        "pd.DataFrame(\n",
        "    data    = modelo_pca.components_,\n",
        "    columns = datos.columns,\n",
        "    index   = ['PC1', 'PC2', 'PC3', 'PC4']\n",
        ")"
      ],
      "metadata": {
        "id": "-J6dbX20_9Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap componentes\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 2))\n",
        "componentes = modelo_pca.components_\n",
        "plt.imshow(componentes.T, cmap='viridis', aspect='auto')\n",
        "plt.yticks(range(len(datos.columns)), datos.columns)\n",
        "plt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1)\n",
        "plt.grid(False)\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "id": "IcAcJCXA_-wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Porcentaje de varianza explicada por cada componente\n",
        "# ==============================================================================\n",
        "print('----------------------------------------------------')\n",
        "print('Porcentaje de varianza explicada por cada componente')\n",
        "print('----------------------------------------------------')\n",
        "print(modelo_pca.explained_variance_ratio_)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "ax.bar(\n",
        "    x      = np.arange(modelo_pca.n_components_) + 1,\n",
        "    height = modelo_pca.explained_variance_ratio_\n",
        ")\n",
        "\n",
        "for x, y in zip(np.arange(len(datos.columns)) + 1, modelo_pca.explained_variance_ratio_):\n",
        "    label = round(y, 2)\n",
        "    ax.annotate(\n",
        "        label,\n",
        "        (x,y),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(0,10),\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
        "ax.set_ylim(0, 1.1)\n",
        "ax.set_title('Porcentaje de varianza explicada por cada componente')\n",
        "ax.set_xlabel('Componente principal')\n",
        "ax.set_ylabel('Por. varianza explicada');"
      ],
      "metadata": {
        "id": "09L6JU9CAAWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Porcentaje de varianza explicada acumulada\n",
        "# ==============================================================================\n",
        "prop_varianza_acum = modelo_pca.explained_variance_ratio_.cumsum()\n",
        "print('------------------------------------------')\n",
        "print('Porcentaje de varianza explicada acumulada')\n",
        "print('------------------------------------------')\n",
        "print(prop_varianza_acum)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "ax.plot(\n",
        "    np.arange(len(datos.columns)) + 1,\n",
        "    prop_varianza_acum,\n",
        "    marker = 'o'\n",
        ")\n",
        "\n",
        "for x, y in zip(np.arange(len(datos.columns)) + 1, prop_varianza_acum):\n",
        "    label = round(y, 2)\n",
        "    ax.annotate(\n",
        "        label,\n",
        "        (x,y),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(0,10),\n",
        "        ha='center'\n",
        "    )\n",
        "\n",
        "ax.set_ylim(0, 1.1)\n",
        "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
        "ax.set_title('Porcentaje de varianza explicada acumulada')\n",
        "ax.set_xlabel('Componente principal')\n",
        "ax.set_ylabel('Por. varianza acumulada');"
      ],
      "metadata": {
        "id": "_2A3bgiZANWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proyección de las observaciones de entrenamiento\n",
        "# ==============================================================================\n",
        "proyecciones = pca_pipe.transform(X=datos)\n",
        "proyecciones = pd.DataFrame(\n",
        "    proyecciones,\n",
        "    columns = ['PC1', 'PC2', 'PC3', 'PC4'],\n",
        "    index   = datos.index\n",
        ")\n",
        "proyecciones.head()"
      ],
      "metadata": {
        "id": "Kc7wCe0gAO-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proyecciones = np.dot(modelo_pca.components_, scale(datos).T)\n",
        "proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])\n",
        "proyecciones = proyecciones.transpose().set_index(datos.index)\n",
        "proyecciones.head()"
      ],
      "metadata": {
        "id": "YIMg30ffARfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recostruccion de las proyecciones\n",
        "# ==============================================================================\n",
        "recostruccion = pca_pipe.inverse_transform(X=proyecciones)\n",
        "recostruccion = pd.DataFrame(\n",
        "                    recostruccion,\n",
        "                    columns = datos.columns,\n",
        "                    index   = datos.index\n",
        ")\n",
        "print('------------------')\n",
        "print('Valores originales')\n",
        "print('------------------')\n",
        "display(recostruccion.head())\n",
        "\n",
        "print('---------------------')\n",
        "print('Valores reconstruidos')\n",
        "print('---------------------')\n",
        "display(datos.head())"
      ],
      "metadata": {
        "id": "hpO8-6X7ATCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJEMPLO 3"
      ],
      "metadata": {
        "id": "lhfKNh5oAtsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "from matplotlib import style\n",
        "style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import multiprocessing\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4wr_KGcNAvqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos\n",
        "# ==============================================================================\n",
        "datos = pd.read_csv('meatspec.csv')\n",
        "datos = datos.drop(columns = datos.columns[0])"
      ],
      "metadata": {
        "id": "S0BHRJTLA4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlación entre columnas numéricas\n",
        "# ==============================================================================\n",
        "\n",
        "def tidy_corr_matrix(corr_mat):\n",
        "    '''\n",
        "    Función para convertir una matriz de correlación de pandas en formato tidy\n",
        "    '''\n",
        "    corr_mat = corr_mat.stack().reset_index()\n",
        "    corr_mat.columns = ['variable_1','variable_2','r']\n",
        "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
        "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
        "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
        "\n",
        "    return(corr_mat)\n",
        "\n",
        "corr_matrix = datos.select_dtypes(include=['float64', 'int']) \\\n",
        "              .corr(method='pearson')\n",
        "display(tidy_corr_matrix(corr_matrix).head(5))"
      ],
      "metadata": {
        "id": "SMsuzramA6Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# División de los datos en train y test\n",
        "# ==============================================================================\n",
        "X = datos.drop(columns='fat')\n",
        "y = datos['fat']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                        X,\n",
        "                                        y.values.reshape(-1,1),\n",
        "                                        train_size   = 0.7,\n",
        "                                        random_state = 1234,\n",
        "                                        shuffle      = True\n",
        "                                    )"
      ],
      "metadata": {
        "id": "5amerHE9A7k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación y entrenamiento del modelo\n",
        "# ==============================================================================\n",
        "modelo = LinearRegression(normalize=True)\n",
        "modelo.fit(X = X_train, y = y_train)"
      ],
      "metadata": {
        "id": "CxCxD47nA-Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones test\n",
        "# ==============================================================================\n",
        "predicciones = modelo.predict(X=X_test)\n",
        "predicciones = predicciones.flatten()\n",
        "\n",
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "rmse_ols = mean_squared_error(\n",
        "            y_true  = y_test,\n",
        "            y_pred  = predicciones,\n",
        "            squared = False\n",
        "           )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {rmse_ols}\")"
      ],
      "metadata": {
        "id": "nMec8rgIBAun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento modelo de regresión precedido por PCA con escalado\n",
        "# ==============================================================================\n",
        "pipe_modelado = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
        "pipe_modelado.fit(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "vQYz4iYnBCW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_modelado.set_params"
      ],
      "metadata": {
        "id": "KQl8lzk4BDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones test\n",
        "# ==============================================================================\n",
        "predicciones = pipe_modelado.predict(X=X_test)\n",
        "predicciones = predicciones.flatten()\n",
        "\n",
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "rmse_pcr = mean_squared_error(\n",
        "            y_true  = y_test,\n",
        "            y_pred  = predicciones,\n",
        "            squared = False\n",
        "           )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {rmse_pcr}\")"
      ],
      "metadata": {
        "id": "IAqJrtBZBFW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid de hiperparámetros evaluados\n",
        "# ==============================================================================\n",
        "param_grid = {'pca__n_components': [1, 2, 4, 6, 8, 10, 15, 20, 30, 50]}\n",
        "\n",
        "# Búsqueda por grid search con validación cruzada\n",
        "# ==============================================================================\n",
        "grid = GridSearchCV(\n",
        "        estimator  = pipe_modelado,\n",
        "        param_grid = param_grid,\n",
        "        scoring    = 'neg_root_mean_squared_error',\n",
        "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
        "        cv         = KFold(n_splits=5, random_state=123),\n",
        "        refit      = True,\n",
        "        verbose    = 0,\n",
        "        return_train_score = True\n",
        "       )\n",
        "\n",
        "grid.fit(X = X_train, y = y_train)\n",
        "\n",
        "# Resultados\n",
        "# ==============================================================================\n",
        "resultados = pd.DataFrame(grid.cv_results_)\n",
        "resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n",
        "    .drop(columns = 'params') \\\n",
        "    .sort_values('mean_test_score', ascending = False) \\\n",
        "    .head(3)"
      ],
      "metadata": {
        "id": "33ggOqrFBG4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico resultados validación cruzada para cada hiperparámetro\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7, 3.84), sharey=True)\n",
        "\n",
        "resultados.plot('param_pca__n_components', 'mean_train_score', ax=ax)\n",
        "resultados.plot('param_pca__n_components', 'mean_test_score', ax=ax)\n",
        "ax.fill_between(resultados.param_pca__n_components.astype(np.float),\n",
        "                resultados['mean_train_score'] + resultados['std_train_score'],\n",
        "                resultados['mean_train_score'] - resultados['std_train_score'],\n",
        "                alpha=0.2)\n",
        "ax.fill_between(resultados.param_pca__n_components.astype(np.float),\n",
        "                resultados['mean_test_score'] + resultados['std_test_score'],\n",
        "                resultados['mean_test_score'] - resultados['std_test_score'],\n",
        "                alpha=0.2)\n",
        "ax.legend()\n",
        "ax.set_title('Evolución del error CV')\n",
        "ax.set_ylabel('neg_root_mean_squared_error');"
      ],
      "metadata": {
        "id": "L1-lLfICC75a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejores hiperparámetros por validación cruzada\n",
        "# ==============================================================================\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
        "print(\"----------------------------------------\")\n",
        "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)"
      ],
      "metadata": {
        "id": "6kz8cQt0C9kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento modelo de regresión precedido por PCA con escalado\n",
        "# ==============================================================================\n",
        "pipe_modelado = make_pipeline(StandardScaler(), PCA(n_components=5), LinearRegression())\n",
        "pipe_modelado.fit(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "t6qrDnZIC-7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones test\n",
        "# ==============================================================================\n",
        "predicciones = pipe_modelado.predict(X=X_test)\n",
        "predicciones = predicciones.flatten()\n",
        "\n",
        "# Error de test del modelo\n",
        "# ==============================================================================\n",
        "rmse_pcr = mean_squared_error(\n",
        "            y_true  = y_test,\n",
        "            y_pred  = predicciones,\n",
        "            squared = False\n",
        "           )\n",
        "print(\"\")\n",
        "print(f\"El error (rmse) de test es: {rmse_pcr}\")"
      ],
      "metadata": {
        "id": "5yvFOFaaDCXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}